{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\belsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains two columns:\n",
    "- category: the category of the email\n",
    "- text: the text of the email\n",
    "\n",
    "The dataset is available at [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection).\n",
    "\n",
    "We will read the dataset into a pandas dataframe then specify the category column as the label and the text column as the feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/spam.csv', encoding='ISO 8859-15')[['category', 'text']]\n",
    "X, y = df['text'], df['category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [G,  ,  ,  , u,  , n,  , l,  ,  ,  , j,  , u, ...\n",
       "1       [O,  , k,  ,  ,  , l,  , r,  ,  ,  , J,  , k, ...\n",
       "2       [F,  , r,  , e,  , e,  ,  ,  , e,  , n,  , r, ...\n",
       "3       [U,  ,  ,  , u,  , n,  ,  ,  ,  ,  ,  ,  , e, ...\n",
       "4       [N,  , h,  ,  ,  , I,  ,  ,  , n,  ,  ,  , h, ...\n",
       "                              ...                        \n",
       "5567    [T,  , h,  ,  ,  ,  ,  , h,  , e,  ,  ,  , 2, ...\n",
       "5568    [W,  , l,  , l,  ,  ,  , ï,  , ¿,  , œ,  ,  , ...\n",
       "5569    [P,  ,  ,  ,  ,  , w,  ,  ,  , n,  ,  ,  ,  , ...\n",
       "5570    [T,  , h,  , e,  ,  ,  , g,  , u,  ,  ,  ,  , ...\n",
       "5571    [R,  , f,  , l,  ,  ,  , I,  ,  ,  , r,  , u, ...\n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    output = [\n",
    "        i for i in text if i not in nltk.corpus.stopwords.words('english')]\n",
    "    return output\n",
    "\n",
    "\n",
    "X = X.apply(remove_punctuation)\n",
    "X = X.apply(remove_stopwords)\n",
    "\n",
    "X\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the TFIDF algorithm to vectorize the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m X\u001b[39m.\u001b[39mdropna(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(analyzer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m, ngram_range\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m), lowercase\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m X \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39;49mfit_transform(X)\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m      4\u001b[0m X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X, columns\u001b[39m=\u001b[39mtfidf\u001b[39m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1273\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1275\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "X.dropna(inplace=True)\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(2, 2), lowercase=True)\n",
    "X = tfidf.fit_transform(X).toarray()\n",
    "X = pd.DataFrame(X, columns=tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a dataset with 3000 features, and we can use it to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 41548)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Naive Bayes classifier to train the model. We will use k-fold cross validation to test the model with k=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(force_alpha=True, alpha=1)\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_test(y_pred: np.ndarray, y_test):\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = matrix.ravel()\n",
    "    return {\n",
    "        'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n",
    "        'accuracy': round(accuracy_score(y_test, y_pred), 3),\n",
    "        'f1': round(f1_score(y_test, y_pred, pos_label='ham'), 3),\n",
    "        'recall': round(recall_score(y_test, y_pred, average=\"binary\", pos_label='ham'), 3),\n",
    "        'precision': round(precision_score(y_test, y_pred, pos_label='ham'), 3),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each fold of the cross validation, we will train the model on the training set and test the model on the test set. We will calculate the accuracy and the root mean squared of the results of each fold. Finally, we will calculate the average of the root mean squared of the results of each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tn': 490, 'fp': 0, 'fn': 20, 'tp': 48}\n",
      "{'tn': 487, 'fp': 0, 'fn': 28, 'tp': 43}\n",
      "{'tn': 478, 'fp': 0, 'fn': 44, 'tp': 35}\n",
      "{'tn': 479, 'fp': 0, 'fn': 43, 'tp': 35}\n",
      "{'tn': 479, 'fp': 0, 'fn': 30, 'tp': 48}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m scores \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m'\u001b[39m: []}\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m kf\u001b[39m.\u001b[39msplit(X):\n\u001b[1;32m----> 3\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49miloc[train_index], X\u001b[39m.\u001b[39miloc[\n\u001b[0;32m      4\u001b[0m         test_index], y\u001b[39m.\u001b[39miloc[train_index], y\u001b[39m.\u001b[39miloc[test_index]\n\u001b[0;32m      5\u001b[0m     model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      6\u001b[0m     y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexing.py:1616\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[39m# a list of integers\u001b[39;00m\n\u001b[0;32m   1615\u001b[0m \u001b[39melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1616\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_list_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1618\u001b[0m \u001b[39m# a single integer\u001b[39;00m\n\u001b[0;32m   1619\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1620\u001b[0m     key \u001b[39m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexing.py:1587\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m \u001b[39mReturn Series values by list or array of integers.\u001b[39;00m\n\u001b[0;32m   1572\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[39m`axis` can only be zero.\u001b[39;00m\n\u001b[0;32m   1585\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1587\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_take_with_is_copy(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1588\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1589\u001b[0m     \u001b[39m# re-raise with different error message\u001b[39;00m\n\u001b[0;32m   1590\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpositional indexers are out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_with_is_copy\u001b[39m(\u001b[39mself\u001b[39m: NDFrameT, indices, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   3895\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3896\u001b[0m \u001b[39m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3897\u001b[0m \u001b[39m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3900\u001b[0m \u001b[39m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3901\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3902\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3903\u001b[0m     \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   3904\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_get_axis(axis)\u001b[39m.\u001b[39mequals(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:3886\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3879\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3880\u001b[0m \u001b[39mInternal version of the `take` allowing specification of additional args.\u001b[39;00m\n\u001b[0;32m   3881\u001b[0m \n\u001b[0;32m   3882\u001b[0m \u001b[39mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3883\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3884\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m-> 3886\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m   3887\u001b[0m     indices,\n\u001b[0;32m   3888\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[0;32m   3889\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   3890\u001b[0m     convert_indices\u001b[39m=\u001b[39;49mconvert_indices,\n\u001b[0;32m   3891\u001b[0m )\n\u001b[0;32m   3892\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:978\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    975\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39mverify)\n\u001b[0;32m    977\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 978\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[0;32m    979\u001b[0m     new_axis\u001b[39m=\u001b[39;49mnew_labels,\n\u001b[0;32m    980\u001b[0m     indexer\u001b[39m=\u001b[39;49mindexer,\n\u001b[0;32m    981\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    982\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    983\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    984\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:751\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m    752\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[0;32m    753\u001b[0m             indexer,\n\u001b[0;32m    754\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[0;32m    757\u001b[0m             ),\n\u001b[0;32m    758\u001b[0m         )\n\u001b[0;32m    759\u001b[0m         \u001b[39mfor\u001b[39;49;00m blk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks\n\u001b[0;32m    760\u001b[0m     ]\n\u001b[0;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:752\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 752\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[0;32m    753\u001b[0m             indexer,\n\u001b[0;32m    754\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[0;32m    757\u001b[0m             ),\n\u001b[0;32m    758\u001b[0m         )\n\u001b[0;32m    759\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[0;32m    760\u001b[0m     ]\n\u001b[0;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:890\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[39mif\u001b[39;00m new_mgr_locs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m     new_mgr_locs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr_locs\n\u001b[1;32m--> 890\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_equal(new_values\u001b[39m.\u001b[39;49mdtype, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype):\n\u001b[0;32m    891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(new_values, new_mgr_locs)\n\u001b[0;32m    892\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\dtypes\\common.py:585\u001b[0m, in \u001b[0;36mis_dtype_equal\u001b[1;34m(source, target)\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, (PeriodDtype, IntervalDtype, CategoricalDtype))\n\u001b[0;32m    582\u001b[0m     \u001b[39mreturn\u001b[39;00m _is_dtype(arr_or_dtype, condition)\n\u001b[1;32m--> 585\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_dtype_equal\u001b[39m(source, target) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m    586\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[39m    Check if two dtypes are equal.\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target, \u001b[39mstr\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = {'accuracy': [], 'f1': [], 'recall': [], 'precision': []}\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[\n",
    "        test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results = score_test(y_pred, y_test)\n",
    "    scores = {k: scores[k] + [results[k]] for k in scores}\n",
    "    print(results['confusion_matrix'])\n",
    "print('Accuracy:', '%.3f +/- %.3f' %\n",
    "      (np.mean(scores['accuracy']), np.std(scores['accuracy'])))\n",
    "print('F1 Score:', '%.3f +/- %.3f' %\n",
    "      (np.mean(scores['f1']), np.std(scores['f1'])))\n",
    "print('Recall:', '%.3f +/- %.3f' %\n",
    "      (np.mean(scores['recall']), np.std(scores['recall'])))\n",
    "print('Precision:', '%.3f +/- %.3f' %\n",
    "      (np.mean(scores['precision']), np.std(scores['precision'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
